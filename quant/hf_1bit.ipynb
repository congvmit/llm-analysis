{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hqq.engine.hf import HQQModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Weight file missing. Check your cache directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Load the model\u001b[39;00m\n\u001b[1;32m      2\u001b[0m model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmobiuslabsgmbh/Llama-2-7b-chat-hf_1bitgs8_hqq\u001b[39m\u001b[38;5;124m'\u001b[39m \n\u001b[0;32m----> 3\u001b[0m model     \u001b[38;5;241m=\u001b[39m \u001b[43mHQQModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_quantized\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43madapter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43madapter_v0.1.lora\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./.cache\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Workspace/LLM/quant/hqq/hqq/engine/base.py:82\u001b[0m, in \u001b[0;36mHQQWrapper.from_quantized\u001b[0;34m(cls, save_dir_or_hub, compute_dtype, device, cache_dir, adapter)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_quantized\u001b[39m(\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     80\u001b[0m ):\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;66;03m# Both local and hub-support\u001b[39;00m\n\u001b[0;32m---> 82\u001b[0m     save_dir \u001b[38;5;241m=\u001b[39m \u001b[43mBaseHQQModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtry_snapshot_download\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_dir_or_hub\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m     arch_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_arch_key_from_save_dir(save_dir)\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_check_arch_support(arch_key)\n",
      "File \u001b[0;32m~/Workspace/LLM/quant/hqq/hqq/models/base.py:290\u001b[0m, in \u001b[0;36mBaseHQQModel.try_snapshot_download\u001b[0;34m(cls, save_dir_or_hub, cache_dir)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# Check\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mget_weight_file(save_dir)):\n\u001b[0;32m--> 290\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWeight file missing. Check your cache directory.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mget_config_file(save_dir)):\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConfig file missing. Check your cache directory.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mException\u001b[0m: Weight file missing. Check your cache directory."
     ]
    }
   ],
   "source": [
    "#Load the model\n",
    "model_id = 'mobiuslabsgmbh/Llama-2-7b-chat-hf_1bitgs8_hqq' \n",
    "model     = HQQModelForCausalLM.from_quantized(model_id, \n",
    "                                               adapter='adapter_v0.1.lora', \n",
    "                                               device='mps', \n",
    "                                               cache_dir=\"./.cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=\"./.cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Inference Mode\n",
    "tokenizer.add_bos_token = False\n",
    "tokenizer.add_eos_token = False\n",
    "\n",
    "if not tokenizer.pad_token: tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "model.config.use_cache  = True\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Optional: torch compile for faster inference\n",
    "# model = torch.compile(model)\n",
    "\n",
    "#Streaming Inference\n",
    "import torch, transformers\n",
    "from threading import Thread\n",
    "\n",
    "def chat_processor(chat, max_new_tokens=100, do_sample=True, device='cuda'):\n",
    "    tokenizer.use_default_system_prompt = False\n",
    "    streamer = transformers.TextIteratorStreamer(tokenizer, timeout=10.0, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "    generate_params = dict(\n",
    "        tokenizer(\"<s> [INST] \" + chat + \" [/INST] \", return_tensors=\"pt\").to(device),\n",
    "        streamer=streamer,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=do_sample,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        top_p=0.90 if do_sample else None,\n",
    "        top_k=50 if do_sample else None,\n",
    "        temperature= 0.6 if do_sample else None,\n",
    "        num_beams=1,\n",
    "        repetition_penalty=1.2,\n",
    "    )\n",
    "\n",
    "    t = Thread(target=model.generate, kwargs=generate_params)\n",
    "    t.start()\n",
    "    \n",
    "    print(\"User: \", chat); \n",
    "    print(\"Assistant: \");\n",
    "    outputs = \"\"\n",
    "    for text in streamer:\n",
    "        outputs += text\n",
    "        print(text, end=\"\", flush=True)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "  \n",
    "    return outputs\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-py3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
